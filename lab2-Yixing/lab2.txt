
########################################################################
#   Lab 2: HMM's and You
#   EECS E6870: Speech Recognition
#   Due: February 26, 2016 at 6pm
########################################################################

* Name: Yixing Chen, yc3094


########################################################################
#   Part 1
########################################################################

* Some people put HMM output distributions on states (e.g., in the readings)
  and some people put HMM output distributions on arcs (e.g., the slides
  and the lab).  Are these two representations equivalent, e.g., can
  they express the same set of models?  Can you think of any
  advantage one representation might have over the other?

->I think both of these two methods are correct. Actually, for most of situations, they are equivalent, but for some situations, one of them may be better and more suitable than another. For example, in this lab2, we suppose there is not skip arc, and we use DP to solve problem, in this case, putting HMM output on arcs is better. But if there are skip arcs, if we still put output on arcs, there may be arcs without HMM output. So in this case, putting HMM output on states is better, we only need to deal with the graph.


* When doing isolated word recognition, one method is to compute the
  likelihood of the acoustic feature vectors with each word HMM separately,
  and then to pick the word HMM with the highest likelihood.
  Another method is to use the "one big HMM" paradigm and
  to use the Viterbi algorithm and traceback to select the best word.
  Are these methods equivalent (in terms of the answer selected)?
  Why or why not?

->Yes, they are equivalent. Actually, we can combine the small HMMs into a big HMM. Add start and end states, then add arcs which connect the states, the small HMMs are combined into a big HMM, and this method will not change the likelihood so that they are totally equivalent in terms of the answer selected.


* To do the dynamic programming computation correctly, one must
  iterate through the cells in the dynamic programming chart
  in an order that satisfies the following property:
  when filling in a cell, all cells that the cell
  depends on must already be filled in.  Consider the following
  two orderings for filling the DP chart:

      (1) for (int frmIdx = 0; frmIdx < frmCnt; ++frmIdx)
            for (int stateIdx = 0; stateIdx < stateCnt; ++stateIdx)
              fill_DP_cell(frmIdx + 1, stateIdx);

      (2) for (int stateIdx = 0; stateIdx < stateCnt; ++stateIdx)
            for (int frmIdx = 0; frmIdx < frmCnt; ++frmIdx)
              fill_DP_cell(frmIdx + 1, stateIdx);

  If there are no skip arcs, which one of these orderings will always
  produce the correct result regardless of HMM topology?  Describe
  a situation where the other ordering can give the wrong answer.
  If there are skip arcs, under what conditions is the good ordering
  still valid?

->The first one is always correct, but the second one may be wrong because sometimes the cell(frmIdx, stateIdx) may not have been computed, which may cause a wrong result. For example, if we want to fill_DP_cell(frmIdx + 1, stateIdx), we should use the data in cell(frmIdx, stateIdx), but the data in this cell has not been computed because the data of this cell is depended on the cell(frmIdx - 1, stateIdx1) (frmIdx - 1 > 0 and stateIdx1 > stateIdx), in this case, the second one is wrong.  


* Create the file "p1b.out" by running:

      lab2_p1b.sh | tee p1b.out

  Submit the following files:

      submit-e6870.py lab2 lab2_vit.C p1b.out

  More generally, the usage of "submit-e6870.py" is as follows:

      submit-e6870.py <lab#> <file1> <file2> <file3> ...

  You can submit a file multiple times; later submissions
  will overwrite earlier ones.  Submissions will fail
  if the destination directory for you has not been created
  for the given <lab#>; contact us if this happens.


########################################################################
#   Part 2
########################################################################

* Create the file "p2b.gmm" by running "lab2_p2b.sh".
  Submit the following files:

      submit-e6870.py lab2 gmm_util.C p2b.gmm


* In this lab, we assumed all GMM's were composed of a single Gaussian.
  When GMM's are composed of multiple Gaussians, each component Gaussian
  of the mixture is updated in essentially the same way as before,
  except we need to figure out the correct posterior counts to use.
  Explain how to compute the posterior count of each component Gaussian
  given the posterior count of the entire GMM.

-> At first, we should use the Equation 9.43 (P143) in Holmes 9.10.2(Baum-Welch re-estimation) to calculate the weight of each component Gaussian, then use Equation 9.44 to compute the posterior count of each component Gaussian (given the posterior count of the entire GMM).


* Given the total posterior counts of each Gaussian in a GMM, explain
  how to reestimate the mixture weights of each Gaussian in that GMM.

-> Use the Equation 9.44 in Holmes 9.10.2(Baum-Welch re-estimation), we know the numerator(the total posterior counts of each Gaussian) and denominator (sum of posterior counts of all states), so we can reestimate the mixture weights of each Gaussian. 


########################################################################
#   Part 3
########################################################################

* Create the file "p3c.out" containing the output of
  running "lab2_p3c.sh" (i.e., run "lab2_p3c.sh | tee p3c.out").
  Submit the following files:

      submit-e6870.py lab2 lab2_fb.C p3c.out


########################################################################
#   Part 4
########################################################################

* What word-error rates did you find by running "lab2_p4a.sh"?

->
96.00% corr (96/100 wds)
0.00% del (0/100 wds)
0.00% ins (0/100 wds)
4.00% sub (4/100 wds)
4.00% WER (4/100 wds)
Elapsed time: 0 sec

* What word-error rates did you find by running "lab2_p4b.sh"?

->
93.97% corr (343/365 wds)
1.10% del (4/365 wds)
15.07% ins (55/365 wds)
4.93% sub (18/365 wds)
21.10% WER (77/365 wds)
Elapsed time: 0 sec


* What word-error rates did you find by running "lab2_p4c.sh"?

->
68.00% corr (68/100 wds)
0.00% del (0/100 wds)
0.00% ins (0/100 wds)
32.00% sub (32/100 wds)
32.00% WER (32/100 wds)
Elapsed time: 0 sec


* What did you learn in this part?

-> In this part, we can know that our HMM/GMM system is better than the DTW system. When it decodes connected digit string data, the error rate will be higher than decoding isolated digits with this model. Besides, We can also see, after including trained transition probabilities by embedding them in the big HMM used for decoding, the error rate will be higher.


* If an HMM were a fruit, what type of fruit would it be?

-> If an HMM were a fruit, I think itâ€™s grape. Because the states in HMM link to each other, just like grape, one to one, link together.


########################################################################
#   Wrap Up
########################################################################

After filling in all of the fields in this file, submit this file
using the following command:

    submit-e6870.py lab2 lab2.txt

The timestamp on the last submission of this file (if you submit
multiple times) will be the one used to determine your official
submission time (e.g., for figuring out if you handed in the
assignment on time).

To verify whether your files were submitted correctly, you can
use the command:

    check-e6870.py lab2

This will list all of the files in your submission directory,
along with file sizes and submission times.


########################################################################
#
########################################################################


